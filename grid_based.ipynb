{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6560764e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "import random\n",
    "import math\n",
    "from dwave.system import DWaveSampler, EmbeddingComposite, DWaveCliqueSampler, FixedEmbeddingComposite\n",
    "import neal\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from minorminer import find_embedding\n",
    "import greedy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec0c179",
   "metadata": {},
   "source": [
    "## Create Work Distribution and Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c207522",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = np.genfromtxt('../wp3')  #Load dataset\n",
    "wp=data[:,2]\n",
    "nproc=2 # Number of processors to partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506d3aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "w00=wp[0:60] #Adjust size of problem as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7bcfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Embeddings \n",
    "final=[]\n",
    "final.append(w00)\n",
    "\n",
    "for kk in range(nproc-1):\n",
    "    w=final[kk] # Store initial WP\n",
    "    h = {} # Empty H \n",
    "    J={}   # Create J\n",
    "    for i in range(len(w)):\n",
    "     for j in range(i+1,len(w)): \n",
    "       J[(i,j)]= w[i]*w[j] #Off diagonal terms only \n",
    "\n",
    "# Combine to form Q\n",
    "Q = {**h, **J}\n",
    "\n",
    "\n",
    "# Target D-Wave hardware graph (e.g., a Chimera graph)\n",
    "sampler = DWaveSampler()  # Initializes a D-Wave Sampler\n",
    "target_graph = sampler.edgelist\n",
    "\n",
    "# Generate the embedding\n",
    "embedding1 = find_embedding(Q.keys(), target_graph)\n",
    "embedding2 = find_embedding(Q.keys(), target_graph)\n",
    "embedding3 = find_embedding(Q.keys(), target_graph)\n",
    "embedding4 = find_embedding(Q.keys(), target_graph)\n",
    "embedding5 = find_embedding(Q.keys(), target_graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cb6afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=============================================Save embeddings \n",
    "#    'embedding1': embedding1,\n",
    "#    'embedding2': embedding2,\n",
    "#    'embedding3': embedding3,\n",
    "#    'embedding4': embedding4,\n",
    "#    'embedding5': embedding5,\n",
    "#}\n",
    "#import pickle\n",
    "#with open('embeddings.pkl', 'wb') as f:\n",
    "#    pickle.dump(embeddings, f)\n",
    "    \n",
    "#==============================================Load embeddings\n",
    "#with open('../embeddings.pkl', 'rb') as f:\n",
    "#    loaded_embeddings = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebb07c3",
   "metadata": {},
   "source": [
    "## Impact of chain strengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96b0357",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run 5 repetitions using embeddings for each chain strength\n",
    "sampler=DWaveSampler(token='')   #Insert token\n",
    "\n",
    "\n",
    "cbf_values = {} #Chainbreak values\n",
    "err_values = {} #Solution Quality\n",
    "\n",
    "cs_norm=max(w00)\n",
    "for cs in [100,500,1000,1500, 2000,3000,6000,10000,20000]:   #Edit Range If Needed\n",
    "     for embedding_name, embedding in [('embedding1', embedding1),('embedding2', embedding2),\n",
    "                                       ('embedding3', embedding3),('embedding4', embedding4),\n",
    "                                       ('embedding5', embedding5)]: #For the prepared embeddings\n",
    "        \n",
    "        final=[]\n",
    "        final.append(w00)\n",
    "        \n",
    "        for kk in range(nproc-1):  #Assuming 2 procs, this loops only triggers once here. In general this is not necessarily  true.\n",
    "            w=final[kk] # Store initial WP\n",
    "            h = {} \n",
    "            J={}   \n",
    "            for i in range(len(w)):\n",
    "             for j in range(i+1,len(w)): \n",
    "               J[(i,j)]= w[i]*w[j] \n",
    "        \n",
    "            sampler_embedded = FixedEmbeddingComposite(sampler, embedding)\n",
    "            sampleset = sampler_embedded.sample_ising(h, J,\n",
    "                                    num_reads = 1000,\n",
    "                                    label='load',\n",
    "                                    chain_strength=cs*cs_norm)\n",
    "\n",
    "            wlist_1=[]      #list of weights\n",
    "            wlist_2=[]\n",
    "\n",
    "            for part, proc in sampleset.first.sample.items(): #Isolate the best solution and create 2 sets (i.e. one for each processor)\n",
    "                if proc == -1 : \n",
    "                    wlist_1.append(w[part])\n",
    "                else :\n",
    "                    wlist_2.append(w[part])\n",
    "\n",
    "\n",
    "            final.append(wlist_1)  # Add new split levels to final (In general can have nested levels for multiple processors) \n",
    "            final.append(wlist_2)\n",
    "            #Store CBF and Error \n",
    "            cbf=sampleset.first.chain_break_fraction                       #calculate cbf\n",
    "            cbf_values.setdefault((cs, embedding_name), []).append(cbf)    #store cbf\n",
    "            err=abs(sum(final[1])-sum(final[2]))/(0.5*(sum(final[1])+sum(final[2]))) #calculate err\n",
    "            err_values.setdefault((cs, embedding_name), []).append(err)              #store err\n",
    "        print(\"Final Output\")\n",
    "        for i in range(len(final)):\n",
    "            print(\"Level=\",i,sum(final[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483260df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Snippet to aggregate then average cbf values across all embeddings for each cs\n",
    "averages_cbf = {}\n",
    "cbf_bars = {} \n",
    "\n",
    "# First, aggregate all kk1 values for each chain strength (cs)\n",
    "aggregated_values = {}\n",
    "for (cs, embedding_name), values in cbf_values.items():\n",
    "    aggregated_values.setdefault(cs, []).extend(values)\n",
    "\n",
    "# Now, compute the averages\n",
    "for cs, values in aggregated_values.items():\n",
    "    avg_kk1 = sum(values) / len(values)\n",
    "    averages_cbf[cs] = avg_kk1\n",
    "#Compute error bars\n",
    "    variance = sum([(x - avg_kk1) ** 2 for x in values]) / len(values)\n",
    "    std_dev = math.sqrt(variance)\n",
    "    cbf_bars[cs] = std_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e601bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Snippet to aagregate then average ERR values across all embeddings for each cs\n",
    "averages_err = {}\n",
    "err_bars = {} \n",
    "# First, aggregate all kk1 values for each chain strength (cs)\n",
    "aggregated_values = {}\n",
    "for (cs, embedding_name), values in err_values.items():\n",
    "    aggregated_values.setdefault(cs, []).extend(values)\n",
    "\n",
    "# Now, compute the averages\n",
    "for cs, values in aggregated_values.items():\n",
    "    avg_kk1 = sum(values) / len(values)\n",
    "    averages_err[cs] = avg_kk1\n",
    "#Compute error bars\n",
    "    variance = sum([(x - avg_kk1) ** 2 for x in values]) / len(values)\n",
    "    std_dev = math.sqrt(variance)\n",
    "    err_bars[cs] = std_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9cfb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot of CBF against CS\n",
    "chain_strengths = list(averages_cbf.keys())\n",
    "mean_values = list(averages_cbf.values())\n",
    "sd_values = list(cbf_bars.values())\n",
    "\n",
    "plt.errorbar(chain_strengths, mean_values, yerr=sd_values, fmt='o', capsize=5, label='Mean with SD')\n",
    "plt.xlabel(\"Chain Strength\",fontweight='bold')\n",
    "plt.ylabel(\"Chain Break Fraction\",fontweight='bold')\n",
    "plt.tight_layout()\n",
    "#plt.savefig(\"cbf_cs.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc5f380",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plt of error agaisnt cs with inset\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "\n",
    "chain_strengths = list(averages_err.keys())\n",
    "mean_values = list(averages_err.values())\n",
    "sd_values = list(err_bars.values())\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Main plot\n",
    "ax.errorbar(chain_strengths, mean_values, yerr=sd_values, fmt='o', capsize=5, label='Mean with SD')\n",
    "plt.xlabel(\"Chain Strength\", fontweight='bold')\n",
    "plt.ylabel(\"Solution Quality\",fontweight='bold')\n",
    "\n",
    "# Define the inset axis\n",
    "axins = inset_axes(ax, width='30%', height='30%', loc='upper right',\n",
    "                  bbox_to_anchor=(-0.01, 0.01, 1, 1), bbox_transform=ax.transAxes)\n",
    "\n",
    "# Plot the last 3 points on the inset axis\n",
    "x_last3 = chain_strengths[-3:]\n",
    "y_last3 = mean_values[-3:]\n",
    "axins.errorbar(chain_strengths[-3:], mean_values[-3:], yerr=sd_values[-3:], fmt='o', capsize=5, label='Mean with SD')\n",
    "axins.set_xlim(min(x_last3)-500, max(x_last3)+500)  # set x limits for clarity\n",
    "axins.set_ylim(min(y_last3)-0.0003, max(y_last3)+0.0005)  # set y limits for clarity\n",
    "\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.savefig(\"err_cs2.pdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5945c746",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plt of error agaisnt cs with inset\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "\n",
    "chain_strengths = list(averages_cbf.keys())\n",
    "mean_values = list(averages_cbf.values())\n",
    "sd_values = list(cbf_bars.values())\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Main plot\n",
    "ax.errorbar(chain_strengths, mean_values, yerr=sd_values, fmt='o', capsize=5, label='Mean with SD')\n",
    "plt.xlabel(\"Chain Strength\", fontweight='bold')\n",
    "plt.ylabel(\"Chain Break Fraction\",fontweight='bold')\n",
    "\n",
    "# Define the inset axis\n",
    "axins = inset_axes(ax, width='30%', height='30%', loc='upper right',\n",
    "                  bbox_to_anchor=(-0.01, 0.01, 1, 1), bbox_transform=ax.transAxes)\n",
    "\n",
    "# Plot the last 3 points on the inset axis\n",
    "x_last3 = chain_strengths[-3:]\n",
    "y_last3 = mean_values[-3:]\n",
    "axins.errorbar(chain_strengths[-3:], mean_values[-3:], yerr=sd_values[-3:], fmt='o', capsize=5, label='Mean with SD')\n",
    "axins.set_xlim(min(x_last3)-500, max(x_last3)+500)  # set x limits for clarity\n",
    "axins.set_ylim(min(y_last3)-0.02, max(y_last3)+0.08)  # set y limits for clarity\n",
    "\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.savefig(\"cbf_cs2.pdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f963324",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chainbreaks vs problem size, computed using the same h/J matrices but default chain strength\n",
    "data_column_names = ['x', 'y']\n",
    "error_column_names = ['x','y_error']\n",
    "\n",
    "# Read the CSV files, assuming no headers\n",
    "# Replace 'data.csv' and 'error_bars.csv' with your actual file names\n",
    "data_df = pd.read_csv('cbf.csv', header=None, names=data_column_names)\n",
    "error_df = pd.read_csv('cbf_err.csv', header=None, names=error_column_names)\n",
    "\n",
    "error= abs(data_df['y'] - error_df['y_error'])\n",
    "#data_df['y']\n",
    "#error_df['y_error']\n",
    "fig=plt.figure(figsize=(8, 6), dpi=100)\n",
    "plt.scatter(data_df['x'],data_df['y'],marker='o',s=70)\n",
    "plt.errorbar(data_df['x'],data_df['y'],yerr=error,linestyle=\"none\", capsize=3)\n",
    "plt.xlabel('Problem Size')\n",
    "plt.ylabel('Chain Break Fraction')\n",
    "#plt.savefig(\"cbf.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c05a4da",
   "metadata": {},
   "source": [
    "## Number of Anneals "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cd4b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding effect of anneals - Loop this code as required over range of desired number of anneals\n",
    "#For each configuration, it will store results in solq_final\n",
    "\n",
    "#Run recursive partition 5 times \n",
    "solq_final=[]\n",
    "for tt in range(5):\n",
    "    final=[]\n",
    "    final.append(w00)\n",
    "\n",
    "    final2=[]\n",
    "\n",
    "    for kk in range(nproc-1):\n",
    "        w=final[kk] # Store initial WP\n",
    "        h = {} # Empty H \n",
    "        J={}   # Create J\n",
    "        for i in range(len(w)):\n",
    "         for j in range(i+1,len(w)): \n",
    "           J[(i,j)]= w[i]*w[j] \n",
    "\n",
    "\n",
    "# Define the sampler\n",
    "#========================================================\n",
    "        sampler = EmbeddingComposite(DWaveSampler())\n",
    "    #sampler = neal.SimulatedAnnealingSampler()\n",
    "    #sampler = greedy.SteepestDescentSolver()\n",
    "# Run the problem on the sampler and print the results\n",
    "        sampleset = sampler.sample_ising(h, J,\n",
    "                                     num_reads = 25,\n",
    "                                     label='Load')\n",
    "\n",
    "    #print(sampleset.first.sample)\n",
    "        plist_1=[]      #list of particles \n",
    "        plist_2=[]\n",
    "        wlist_1=[]      #list of weights\n",
    "        wlist_2=[]\n",
    "\n",
    "        for part, proc in sampleset.first.sample.items():\n",
    "        #print(part,\":\",proc)\n",
    "            if proc == -1 : \n",
    "                plist_1.append(part)\n",
    "                wlist_1.append(w[part])\n",
    "            else :\n",
    "                plist_2.append(part)\n",
    "                wlist_2.append(w[part])\n",
    "\n",
    "\n",
    "        final.append(wlist_1)  # Add new split levels to final \n",
    "        final.append(wlist_2)\n",
    "\n",
    "    print(\"Final Output\")\n",
    "    for i in range(len(final)):\n",
    "        print(\"Level=\",i,sum(final[i]))\n",
    "    #print(\"Level=\",i,sum(final[i]), file=open('test_gr.dat', 'a'))\n",
    "    print(plist_1)\n",
    "    print(plist_2)\n",
    "    df=sampleset.to_pandas_dataframe()\n",
    "    out=\"b25_\"+str(tt+1)+\".dat\"\n",
    "    solq=abs(sum(final[1])-sum(final[2]))/(0.5*(sum(final[1])+sum(final[2])))\n",
    "    solq_final.append(solq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd16b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine precomputed results\n",
    "solq_final_vars = [solq_final, solq2_final, solq3_final, solq4_final, solq5_final, solq6_final, solq7_final, solq8_final]\n",
    "\n",
    "# Calculate mean and std\n",
    "c1 = [np.mean(solq) for solq in solq_final_vars]\n",
    "s1 = [np.std(solq) for solq in solq_final_vars]\n",
    "\n",
    "# Define x-axis values i.e. number of anneals\n",
    "x = [25, 50, 100, 200, 500, 1000, 1500, 2000]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(x, c1, label=\"Mean Solution Quality\", color='blue', zorder=5)\n",
    "plt.errorbar(x, c1, yerr=s1, linestyle=\"none\", capsize=3, color='blue', zorder=4)\n",
    "plt.ylabel(\"Solution Quality\")\n",
    "plt.xlabel(\"Number of Anneals\")\n",
    "plt.title(\"Effect of Number of Anneals on Solution Quality\")\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "# Save plot to a file\n",
    "#plt.savefig(\"number_anneals.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab669823",
   "metadata": {},
   "source": [
    "## Likelihood of good solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d276f129",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Round Robin Results\n",
    "\n",
    "def round_robin_partition2(numbers):\n",
    "    num_partitions = 2\n",
    "    partitions = [[] for _ in range(num_partitions)]  # Create empty partitions\n",
    "\n",
    "    for i, num in enumerate(sorted(numbers)):\n",
    "        partition_index = i % num_partitions  # Determine the partition index using modulo operation\n",
    "        partitions[partition_index].append(num)  # Add the number to the corresponding partition\n",
    "\n",
    "    return partitions\n",
    "data = np.genfromtxt('../wp2')  ##Choose a dataset i.e. wp2 or wp3\n",
    "numbers=data[:,2]\n",
    "numbers=numbers[0:60] #Truncate problem size\n",
    "\n",
    "#Recursively apply RR\n",
    "result = round_robin_partition2(numbers)\n",
    "\n",
    "\n",
    "print(\"Partition 1:\", sum(result[0]))\n",
    "print(\"Partition 2:\", sum(result[1]))\n",
    "\n",
    "zz=(sum(result[1])-sum(result[0]))/(sum(result[0])+sum(result[1])) #needed for plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d6a837",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Original Recursive Partition for single run and single problem size\n",
    "\n",
    "data = np.genfromtxt('../wp2')  #Choose a dataset i.e. wp2 or wp3\n",
    "w00=data[:,2]\n",
    "w00=w00[0:60] #Truncate problem size\n",
    "\n",
    "final=[]\n",
    "final.append(w00)\n",
    "for kk in range(nproc-1):\n",
    "    w=final[kk] # Store initial WP\n",
    "    h = {} # Empty H \n",
    "    J={}   # Create J\n",
    "    for i in range(len(w)):\n",
    "     for j in range(i+1,len(w)): \n",
    "       J[(i,j)]= w[i]*w[j] #\n",
    "\n",
    "\n",
    "# Define the sampler that will be used to run the problem\n",
    "#========================================================\n",
    "    sampler = EmbeddingComposite(DWaveSampler())\n",
    "\n",
    "    #sampler = greedy.SteepestDescentSolver()\n",
    "# Run the problem on the sampler and print the results\n",
    "#    sampleset = sampler.sample_ising(h, J,\n",
    "#                                 num_reads = 100,\n",
    "#                                 label='Load')\n",
    "    sampleset = sampler.sample_ising(h, J,\n",
    "                                 num_reads = 100,\n",
    "                                 label='load',chain_strength=100000)  #Adjust chainstrenght if needed\n",
    "\n",
    "    #print(sampleset.first.sample)\n",
    "    plist_1=[]      #list of particles \n",
    "    plist_2=[]\n",
    "    wlist_1=[]      #list of weights\n",
    "    wlist_2=[]\n",
    "\n",
    "    for part, proc in sampleset.first.sample.items():\n",
    "        #print(part,\":\",proc)\n",
    "        if proc == -1 : \n",
    "            plist_1.append(part)\n",
    "            wlist_1.append(w[part])\n",
    "        else :\n",
    "            plist_2.append(part)\n",
    "            wlist_2.append(w[part])\n",
    "\n",
    "\n",
    "    final.append(wlist_1)  # Add new split levels to final \n",
    "    final.append(wlist_2)\n",
    "\n",
    "print(\"Final Output\")\n",
    "for i in range(len(final)):\n",
    "    print(\"Level=\",i,sum(final[i]))\n",
    "    #print(\"Level=\",i,sum(final[i]), file=open('test_gr.dat', 'a'))\n",
    "print(plist_1)\n",
    "print(plist_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9e8e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "c5=sampleset.to_pandas_dataframe()\n",
    "sol_std=[]\n",
    "sol_org=c5.sort_values(by=\"energy\",ascending=True)\n",
    "for j in range(0,len(sol_org)):      # Looping over solutions\n",
    "    \n",
    "    sol=sol_org.iloc[j]\n",
    "    sol_t1=[]\n",
    "    sol_t2=[]\n",
    "    sol_final=[]\n",
    "\n",
    "    for i in range(0,len(w00)):   #Looping over patches\n",
    "        if (sol[i] < 0):\n",
    "            sol_t1.append(w[i])\n",
    "        else:\n",
    "            sol_t2.append(w[i])\n",
    "    sol_final=[]\n",
    "    sol_final.append(sum(sol_t1))\n",
    "    sol_final.append(sum(sol_t2))\n",
    "    #sol_std.append(np.std(sol_final))  \n",
    "    sol_std.append(abs(sol_final[0]-sol_final[1])/sum(sol_final))\n",
    "#sol_std=abs(sol_final[0]-sol_final[1])/sum(sol_final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3834f036",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(sol_org[\"energy\"], sol_std,marker=\"o\",label=\"QA\")\n",
    "plt.xlabel(\"Energy\")\n",
    "plt.ylabel(\"Solution Imbalance\")\n",
    "#plt.title(\"Solution Quality for all samples of Case C5\")b\n",
    "plt.axhline(y=0,color=\"r\",linestyle=\"--\",label=\"SD\") #SD case-previous code block yields perfect solution i.e. y=o\n",
    "plt.axhline(y=zz,color=\"k\",linestyle=\"--\",label=\"RR\")\n",
    "plt.legend()\n",
    "#plt.savefig(\"solution_qaulity.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5506fe",
   "metadata": {},
   "source": [
    "## Supplementary Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e70d431",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Round Robin Results \n",
    "\n",
    "def round_robin_partition2(numbers):\n",
    "    num_partitions = 2\n",
    "    partitions = [[] for _ in range(num_partitions)]  # Create empty partitions\n",
    "\n",
    "    for i, num in enumerate(sorted(numbers)):\n",
    "        partition_index = i % num_partitions  # Determine the partition index using modulo operation\n",
    "        partitions[partition_index].append(num)  # Add the number to the corresponding partition\n",
    "\n",
    "    return partitions\n",
    "data = np.genfromtxt('../wp2')  #Load dataset\n",
    "numbers=data[:,2]\n",
    "numbers=numbers[0:100]\n",
    "\n",
    "#Recursively apply RR \n",
    "result = round_robin_partition2(numbers)\n",
    "\n",
    "result2a=round_robin_partition2(result[0])\n",
    "result2b=round_robin_partition2(result[1])\n",
    "\n",
    "result3a=round_robin_partition2(result2a[0])\n",
    "result3b=round_robin_partition2(result2a[1])\n",
    "result3c=round_robin_partition2(result2b[0])\n",
    "result3d=round_robin_partition2(result2b[1])\n",
    "\n",
    "result4a=round_robin_partition2(result3a[0])\n",
    "result4b=round_robin_partition2(result3a[1])\n",
    "result4c=round_robin_partition2(result3b[0])\n",
    "result4d=round_robin_partition2(result3b[1])\n",
    "result4e=round_robin_partition2(result3c[0])\n",
    "result4f=round_robin_partition2(result3c[1])\n",
    "result4g=round_robin_partition2(result3d[0])\n",
    "result4h=round_robin_partition2(result3d[1])\n",
    "print(\"Partition 1:\", sum(result[0]))\n",
    "print(\"Partition 2:\", sum(result[1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f721bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load Data from Files\n",
    "read1 = pd.read_csv(\"../../test_qa.dat\", delim_whitespace=True, header=None, names=[\"a\", \"b\", \"c\"], comment=\"#\")\n",
    "read2 = pd.read_csv(\"../../test_sa.dat\", delim_whitespace=True, header=None, names=[\"a\", \"b\", \"c\"], comment=\"#\")\n",
    "read3 = pd.read_csv(\"../../test_gr.dat\", delim_whitespace=True, header=None, names=[\"a\", \"b\", \"c\"], comment=\"#\")\n",
    "\n",
    "# Extract the 'c' column data for each dataset\n",
    "dd, dd2, dd3 = read1[\"c\"], read2[\"c\"], read3[\"c\"]\n",
    "\n",
    "# Define x-axis values for different processor levels\n",
    "x1, x2, x3, x4 = [1, 2], [1, 2, 3, 4], [1, 2, 3, 4, 5, 6, 7, 8], [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
    "\n",
    "# Define split levels for each dataset\n",
    "#This is level 0 i.e. Total Work\n",
    "split_t, split2_t, split3_t = dd[0], dd2[0], dd3[0]\n",
    "#This is level 1 i.e. 2 procs\n",
    "split_1, split2_1, split3_1 = (dd[1:3] - split_t / 2) / split_t * 100, (dd2[1:3] - split2_t / 2) / split2_t * 100, (dd3[1:3] - split3_t / 2) / split3_t * 100\n",
    "#This is level 2 i.e. 4 procs\n",
    "split_2, split2_2, split3_2 = (dd[3:7] - split_t / 4) / split_t * 100, (dd2[3:7] - split2_t / 4) / split2_t * 100, (dd3[3:7] - split3_t / 4) / split3_t * 100\n",
    "#This is level 3 i.e. 8 procs\n",
    "split_3, split2_3, split3_3 = (dd[7:15] - split_t / 8) / split_t * 100, (dd2[7:15] - split2_t / 8) / split2_t * 100, (dd3[7:15] - split3_t / 8) / split3_t * 100\n",
    "#This is level 4 i.e. 16 procs\n",
    "split_4, split2_4, split3_4 = (dd[15:31] - split_t / 16) / split_t * 100, (dd2[15:31] - split2_t / 16) / split2_t * 100, (dd3[15:31] - split3_t / 16) / split3_t * 100\n",
    "\n",
    "# Calculate sum ranges (ss1 - ss4) for each partition level\n",
    "ss1 = [(sum(result[0]) - split_t / 2) / split_t * 100, (sum(result[1]) - split_t / 2) / split_t * 100]\n",
    "ss2 = [(sum(result2a[0]) - split_t / 4) / split_t * 100, (sum(result2a[1]) - split_t / 4) / split_t * 100, (sum(result2b[0]) - split_t / 4) / split_t * 100, (sum(result2b[1]) - split_t / 4) / split_t * 100]\n",
    "ss3 = [(sum(result3a[0]) - split_t / 8) / split_t * 100, (sum(result3a[1]) - split_t / 8) / split_t * 100, (sum(result3b[0]) - split_t / 8) / split_t * 100, (sum(result3b[1]) - split_t / 8) / split_t * 100,\n",
    "       (sum(result3c[0]) - split_t / 8) / split_t * 100, (sum(result3c[1]) - split_t / 8) / split_t * 100, (sum(result3d[0]) - split_t / 8) / split_t * 100, (sum(result3d[1]) - split_t / 8) / split_t * 100]\n",
    "ss4 = [(sum(result4a[0]) - split_t / 16) / split_t * 100, (sum(result4a[1]) - split_t / 16) / split_t * 100, (sum(result4b[0]) - split_t / 16) / split_t * 100, (sum(result4b[1]) - split_t / 16) / split_t * 100,\n",
    "       (sum(result4c[0]) - split_t / 16) / split_t * 100, (sum(result4c[1]) - split_t / 16) / split_t * 100, (sum(result4d[0]) - split_t / 16) / split_t * 100, (sum(result4d[1]) - split_t / 16) / split_t * 100,\n",
    "       (sum(result4e[0]) - split_t / 16) / split_t * 100, (sum(result4e[1]) - split_t / 16) / split_t * 100, (sum(result4f[0]) - split_t / 16) / split_t * 100, (sum(result4f[1]) - split_t / 16) / split_t * 100,\n",
    "       (sum(result4g[0]) - split_t / 16) / split_t * 100, (sum(result4g[1]) - split_t / 16) / split_t * 100, (sum(result4h[0]) - split_t / 16) / split_t * 100, (sum(result4h[1]) - split_t / 16) / split_t * 100]\n",
    "\n",
    "# Plot scatter for each partition level\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "plt.tick_params(axis='both', which='major', labelsize=16)\n",
    "\n",
    "# Scatter plots for each level\n",
    "for i, (x, splits, ss) in enumerate(zip([x1, x2, x3, x4], [(split_1, split2_1, split3_1), (split_2, split2_2, split3_2), (split_3, split2_3, split3_3), (split_4, split2_4, split3_4)], [ss1, ss2, ss3, ss4])):\n",
    "    fig = plt.figure(figsize=(8, 6), dpi=100)\n",
    "    plt.scatter(x, splits[0], marker=\"+\", label=\"QA\", s=100)\n",
    "    plt.scatter(x, splits[1], marker=\".\", label=\"SA\", s=100)\n",
    "    plt.scatter(x, splits[2], marker=\"x\", label=\"SD\", s=100)\n",
    "    plt.scatter(x, ss, marker=\"1\", label=\"RR\", s=100)\n",
    "    plt.axhline(0, color=\"k\", linestyle=\"--\")\n",
    "    plt.xlabel(\"Proc ID\")\n",
    "    plt.xticks(x)\n",
    "    plt.ylabel(\"Imbalance\")\n",
    "    plt.legend()\n",
    "    # plt.savefig(f\"split{i+1}v4.pdf\", bbox_inches=\"tight\")\n",
    "\n",
    "# Calculate and plot the range of imbalance across methods\n",
    "range_data = {'QA': [], 'SA': [], 'SD': [], 'RR': []}\n",
    "method_data = {\n",
    "    'QA': [split_1, split_2, split_3, split_4],\n",
    "    'SA': [split2_1, split2_2, split2_3, split2_4],\n",
    "    'SD': [split3_1, split3_2, split3_3, split3_4],\n",
    "    'RR': [ss1, ss2, ss3, ss4]\n",
    "}\n",
    "\n",
    "for label, splits in method_data.items():\n",
    "    range_data[label] = [max(kk) - min(kk) for kk in splits]\n",
    "\n",
    "# Plot range comparison\n",
    "xticks = [\"2\", \"4\", \"8\", \"16\"]\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(xticks, range_data['QA'], marker=\"+\", label=\"QA\")\n",
    "plt.plot(xticks, range_data['SA'], marker=\".\", label=\"SA\")\n",
    "plt.plot(xticks, range_data['SD'], marker=\"x\", label=\"SD\")\n",
    "plt.plot(xticks, range_data['RR'], marker=\"1\", label=\"RR\")\n",
    "plt.xlabel(\"Processors\")\n",
    "plt.ylabel(\"Range\")\n",
    "\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "# plt.savefig(\"range.png\", bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ocean",
   "language": "python",
   "name": "ocean"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
